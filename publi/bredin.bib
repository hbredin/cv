# =============================================================================
# TEMPLATES
# =============================================================================


inproceedings{TemplateConference,
	Title = {{Title}},
	Author = {Herv{\'e} Bredin},
	Booktitle = {{Conference}},
	Pages = {},
	Year = {},
	Month = {},
	Address = {City, Country},
	Abstract = {},
}

article{TemplateJournal,
	Title = {{Title}},
	Author = {Herv\'{e} Bredin},
	Journal = {{}},
	Year = {},
	Abstract = {},
}

inbook{TemplateChapter,
	Title = {{Title}},
	Author = {Herv{\'e} Bredin},
	Booktitle = {{}},
	Editor = {{}},
	Month = {},
	Year = {},
	Publisher = {{}},
}


# =============================================================================
# SUBMITTED
# =============================================================================



# =============================================================================
# ACCEPTED
# =============================================================================

# ======= 2015-09 --> 2016-08 =================================================

# ICMI 2015
@inproceedings{Bruneau2015,
	Abstract = {Classification quality criteria such as precision, recall, and F-measure are generally the basis for evaluating contributions in automatic speaker recognition. Specifically, comparisons are carried out mostly via mean values estimated on a set of media. Whilst this approach is relevant to assess improvement w.r.t. the state-of-the-art, or ranking participants in the context of an automatic annotation challenge, it gives little insight to system designers in terms of cues for improving algorithms, hypothesis formulation, and evidence display. This paper presents a design study of a visual and interactive approach to analyze errors made by automatic annotation algorithms. A timeline-based tool emerged from prior steps of this study. A critical review, driven by user interviews, exposes caveats and refines user objectives. The next step of the study is then initiated by sketching designs combining elements of the current prototype to principles newly identified as relevant.},
	Title = {{A Visual Analytics Approach to Finding Factors Improving Automatic Speaker Identification}},
	Author = {Pierrick Bruneau and Micka{\"e}l Stefas and Herv\'{e} Bredin and Johann Poignant and Thomas Tamisier and Claude Barras},
	Booktitle = {ICMI 2015, 17th International Conference on Multimodal Interaction},
	Year = {2015},
	Month = {November},
	Address = {Seattle, USA},
  }

# MediaEval 2015
@inproceedings{Poignant2015a,
	Abstract = {This paper describes the algorithm tested by the LIMSI team in the MediaEval 2015 Person Discovery in Broadcast TV Task. For this task we used an audio/video diarization process constrained by names written on screen. These names are used to both identify clusters and prevent the fusion of two clusters with different co-occurring names. This method obtained 83.1% of EwMAP tuned on the out-domain development corpus.},
	Title = {{LIMSI @ MediaEval 2015: Person Discovery in Broadcast TV Task}},
	Author = {Johann Poignant and Herv\'{e} Bredin and Claude Barras},
	Booktitle = {MediaEval 2015},
	Year = {2015},
	Month = {September},
	Address = {Wurzen, Germany},
	}

# MediaEval 2015
@inproceedings{Poignant2015,
	Abstract = {We describe the ``Multimodal Person Discovery in Broadcast TV'' task of MediaEval 2015 benchmarking initiative. Participants are asked to return the names of people who can be both seen as well as heard in every shot of a collection of videos. The list of people is not known a priori and their names must be discovered in an unsupervised way from media content using text overlay or speech transcripts. The task is evaluated using information retrieval metrics, based on a posteriori collaborative annotation of the test corpus.},
	Title = {{Multimodal Person Discovery in Broadcast TV at MediaEval 2015}},
	Author = {Johann Poignant and Herv\'{e} Bredin and Claude Barras},
	Booktitle = {MediaEval 2015},
	Year = {2015},
	Month = {September},
	Address = {Wurzen, Germany},
	}

# InterSpeech 2015 (show-and-tell)
@inproceedings{Budnik2015,
	Abstract = {This paper presents a collaborative annotation framework for person identification in TV shows. The web annotation front-end will be demonstrated during the Show and Tell session. All the code for annotation is made available on github. The tool can also be used in a crowd-sourcing environment.},
	Title = {{Collaborative Annotation for Person Identification in TV Shows}},
	Author = {Matheuz Budnik and Laurent Besacier and Johann Poignant and Herv\'{e} Bredin and Claude Barras and Micka{\"e}l Stefas and Pierrick Bruneau and Thomas Tamisier},
	Booktitle = {Interspeech 2015, 16th Annual Conference of the International Speech Communication Association},
	Year = {2015},
	Month = {September},
	Address = {Dresden, Germany},
	}

# InterSpeech 2015
@inproceedings{Knyazeva2015,
		Abstract = {Though radio and TV broadcast are highly structured documents, state-of-the-art speaker identification algorithms do not take advantage of this information to improve prediction performance: speech turns are usually identified independently from each other, using unstructured multi-class classification approaches. In this work, we propose to address speaker identification as a sequence labeling task and use two structured prediction techniques to account for the inherent temporal structure of interactions between speakers: the first one relies on Conditional Random Field and can take into account local relations between two consecutive speech turns; the second one, based on the SEARN framework, sacrifices exact inference for the sake of the expressiveness of the model and is able to incorporate rich structure information during prediction. Experiments performed on The Big Bang Theory TV series show that structured prediction techniques outperform the  standard unstructured approach.},
		Title = {{Structured Prediction for Speaker Identification in TV Series}},
		Author = {Elena Knyazeva and Guillaume Wisniewski and Herv\'{e} Bredin and Fran\c{c}ois Yvon},
		Booktitle = {Interspeech 2015, 16th Annual Conference of the International Speech Communication Association},
		Year = {2015},
		Month = {September},
		Address = {Dresden, Germany},
		}

# ERRARE 2015
@inproceedings{Charlet2015,
	Abstract = {Speaker identification approaches for TV broadcast are usually evaluated and compared based on global error rates derived from the overall duration of missed detection, false alarm and confusion. Based on the analysis of the output of the systems submitted to the final round of the French evaluation campaign REPERE, this paper highlights the fact that these average metrics lead to the incorrect intuition that current state-of-the-art algorithms partially recognize all speakers. Setting aside incorrect diarization and adverse acoustic conditions, we show that their performance is in fact essentially bi-modal: in a given show, either all speech turns of a speaker are correctly identified or none of them are. We then proceed with trying to understand and explain this behavior, through perfomance prediction experiments. These experiments show that the most discriminant speaker characteristics are -- first -- their total speech duration in the current show and -- then only -- the amount of training data available to build their acoustic model.},
	Title = {{What Makes a Speaker Recognizable in TV Broadcast? Going Beyond Speaker Identification Error Rate}},
	Author = {Delphine Charlet and Johann Poignant and Herv{\'e} Bredin and Corinne Fredouille and Sylvain Meignier},
	Year = {2015},
	Month = {September},
	Booktitle = {ERRARE 2015, Second Workshop on Errors By Humans and Machines in Multimedia, Multimodal, and Multilingual Data Processing},
	Address = {Sinaia, Romania},
}

@inproceedings{Bredin2014b,
	Abstract = {We address the problem of speaker identification in multimedia data, and TV series in particular. While speaker identification is traditionally a supervised machine-learning task, our first contribution is to significantly reduce the need for costly preliminary manual annotations through the use of automatically aligned (and potentially noisy) fan-generated transcripts and subtitles. We show that both speech activity detection and speech turn identification modules trained in this weakly supervised manner achieve similar performance as their fully supervised counterparts (i.e. relying on fine manual speech/non-speech/speaker annotation). Our second contribution relates to the use of multilingual audio tracks usually available with this kind of content to significantly improve the overall speaker identification performance. Reproducible experiments (including dataset, manual annotations and source code) performed on the first six episodes of The Big Bang Theory TV series show that combining the French audio track (containing dubbed actor voices) with the English one (with the original actor voices) improves the overall English speaker identification performance by 5% absolute and up to 70% relative on the five main characters.},
	Title = {{"Sheldon speaking, bonjour!": Leveraging Multilingual Tracks for (Weakly) Supervised Speaker Identification}},
	Author = {Herv\'{e} Bredin and Anindya Roy and Nicolas P\'{e}cheux and Alexandre Allauzen},
	Booktitle = {ACM MM 2014, 22nd ACM International Conference on Multimedia},
	Year = {2014},
	Month = {November},
	Address = {Orlando, USA},
	}

# ======= 2014-09 --> 2015-08 =================================================

# MediaEval 2014
@inproceedings{Guinaudeau2014,
	Abstract = {This paper provides an overview of the Social Event Detection (SED) system developed at LIMSI for the 2014 campaign. Our approach is based on a hierarchical agglomerative clustering that uses textual metadata, user-based knowledge and geographical information. These different sources of knowledge, either used separately or in cascade, reach good results for the full clustering subtask with a normalized mutual information equal to 0.95 and F1 scores greater than 0.82 for our best run.},
	Title = {{LIMSI @ MediaEval SED 2014}},
	Author = {Camille Guinaudeau and Antoine Laurent and Herv\'{e} Bredin},
	Booktitle = {MediaEval 2014},
	Year = {2014},
	Month = {September},
	Address = {Barcelona, Spain},
	}

# CDVE 2014
@inproceedings{Bruneau2014a,
	Abstract = {Reference multimedia corpora for use in automated indexing algorithms require lots of manual work. The Camomile project advocates the joint progress of automated annotation methods and tools for improving the benchmark resources. This paper shows some work in progress in interactive visualization of annotations, and perspectives in harnessing the collaboration between manual annotators, algorithm designers, and benchmark administrators.},
	Title = {{Collaborative Annotation of Multimedia Resources}},
	Author = {Pierrick Bruneau and Micka{\"e}l Stefas and Mateusz Budnik and Johann Poignant and Herv{\'e} Bredin and Thomas Tamisier and Beno{\^i}t Otjacques},
	Booktitle = {{CDVE 2014, 11th International Conference on Cooperative Design, Visualization and Engineering}},
	Pages = {163--166},
	Year = {2014},
	Month = {September},
	Address = {Seattle, USA},
	Abstract = {Reference multimedia corpora for use in automated annotation algorithms are very demanding of manual work. The Camomile project advocates the joint progress of automated annotation methods and tools for improving the benchmark resources. This paper shows some work in progress in interactive visualization of annotations, and perspectives in harnessing the collaboration between manual annotators, algorithm designers, and benchmark administrators.},
}

# ======= 2013-09 --> 2014-08 =================================================

# IV 2014
@inproceedings{Bruneau2014,
	Title = {{A Web-based Tool for the Visual Analysis of Media Annotations}},
	Author = {Pierrick Bruneau and Micka{\"e}l Stefas and Herv{\'e} Bredin and Anh-Phuong Ta and Thomas Tamisier and Claude Barras},
	Booktitle = {{iV 2014, 18th International Conference Information Visualisation}},
	Year = {2014},
	Month = {July},
	Address = {Paris, France},
	Abstract = {Multimedia annotation algorithms infer localized meta-data in multimedia content, e.g. speakers or appearing faces. There is a growing need of experts from this domain to perform advanced analyses, that go beyond medium-scale quality metrics. This paper describes a novel visual tool, that applies interactive visualization principles to the multimedia expert concerns. Multiple coordinated views, augmented by interactive inspection facilities, ease the navigation in media annotations, and the visual detection of relevant information. The effectiveness of the proposition is demonstrated by experimental scenarios on a real multimedia corpus.},
}

# chapter - fusion
@inbook{Strat2014,
	Abstract = {Current research shows that the detection of semantic concepts (animal, bus, person, dancing etc.) in multimedia documents such as videos, requires the use of several types of complementary descriptors in order to achieve good results. In this work, we explore strategies for combining dozens of complementary content descriptors (or ``experts'') in an efficient way, through the use of late fusion approaches, for concept detection in multimedia documents. We explore two fusion approaches that share a common structure: both start with a clustering of experts stage, continue with an intra-cluster fusion and finish with an inter-cluster fusion, and we also experiment with other state-of-the-art methods. The first fusion approach relies on a priori knowledge about the internals of each expert to group the set of available experts by similarity. The second approach automatically obtains measures on the similarity of experts from their output to group the experts using agglomerative clustering, and then combines the results of this fusion with those from other methods. In the end, we show that an additional performance boost can be obtained by also considering the context of multimedia elements.},
	Author = {Sabin Tiberius Strat and Alexandre Benoit and Patrick Lambert and Herv{\'e} Bredin and Georges Qu\'{e}not},
	Booktitle = {{Fusion in Computer Vision -- Understanding Complex Visual Content}},
	Editor = {Bogdan Ionescu and Jenny Benois-Pineau and Tomas Piatrik and  Georges Qu\'{e}not},
	Month = {},
	Publisher = {Springer Verlag},
	Title = {{Hierarchical Late Fusion for Concept Detection in Videos}},
	Year = {2014},
}

# Odyssey 2014
@inproceedings{Bredin2014a,
	Title = {{Person Instance Graphs for Named Speaker Identification in TV Broadcast}},
	Author = {Herv{\'e} Bredin and Antoine Laurent and Achintya Sarkar and Viet-Bac Le and Sophie Rosset and Claude Barras},
	Booktitle = {{Odyssey 2014, The Speaker and Language Recognition Workshop}},
	Pages = {},
	Year = {2014},
	Month = {June},
	Address = {Joensuu, Finland},
	Abstract = {We address the problem of named speaker identification in TV broadcast which consists in answering the question ''who speaks when?'' with the real identity of speakers, using person names automatically obtained from speech transcripts. While existing approaches rely on a first speaker diarization step followed by a local name propagation step to speaker clusters, we propose a unified framework called person instance graph where both steps are jointly modeled as a global optimization problem, then solved using integer linear programming. Moreover, when available, acoustic speaker models can be added seamlessly to the graph structure for joint named and acoustic speaker identification - leading to a 10% error decrease (from 45% down to 35%) over a state-of-the-art i-vector speaker identification system on the REPERE TV broadcast corpus.},
}

# LREC - TVD
@inproceedings{Roy2014a,
	Abstract = {We present a new dataset built around two TV series, The Big Bang Theory (a situation comedy) and Game of Thrones (a fantasy drama). It has multiple tracks including dialogue, crowd-sourced textual descriptions and metadata, all time-stamped and temporally aligned with each other. We provide tools to reproduce it for research purposes, provided that one has legally acquired the DVDs of the series. The alignment algorithm used is evaluated on a manually aligned subset of the data.},
	Title = {{TVD: a Reproducible and Multiply Aligned TV Series Dataset}},
	Author = {Anindya Roy and Camille Guinaudeau and Herv{\'e} Bredin and Claude Barras},
	Booktitle = {{LREC 2014, 9th Language Resources and Evaluation Conference}},
	Pages = {},
	Year = {2014},
	Month = {May},
	Address = {Reykjavik, Iceland},
}

# MMIR - PIG
@article{Bredin2014,
	Abstract = {This work introduces a unified framework for mono-, cross- and multi-modal person recognition in multimedia data. Dubbed Person Instance Graph, it models the person recognition task as a graph mining problem: i.e. finding the best mapping between person instance vertices and identity vertices. Practically, we describe how the approach can be applied to speaker identification in TV broadcast. Then, a solution to the above-mentioned mapping problem is proposed. It relies on Integer Linear Programming to model the problem of clustering person instances based on their identity. We provide an in-depth theoretical definition of the optimization problem. Moreover, we improve two fundamental aspects of our previous related work: the problem constraints and the optimized objective function. Finally, a thorough experimental evaluation of the proposed framework is performed on a publicly available benchmark database. Depending on the graph configuration (i.e. the choice of its vertices and edges), we show that multiple tasks can be addressed interchangeably (e.g. speaker diarization, supervised or unsupervised speaker identification), significantly outperforming state-of-the-art mono-modal approaches.},
	Author = {Herv\'{e} Bredin and Anindya Roy and Viet-Bac Le and Claude Barras},
	Journal = {International Journal of Multimedia Information Retrieval},
	Publisher = {Springer-Verlag},
	Title = {{Person Instance Graphs for Mono-, Cross- and Multi-Modal Person Recognition in Multimedia Data. Application to Speaker Identification in TV Broadcast}},
	Year = {2014},
}

# MTAP - Lexical SID
@article{Roy2014,
	Abstract = {It is possible to use lexical information extracted from speech transcripts for speaker identification (SID), either on its own or to improve the performance of standard cepstral-based SID systems upon fusion. This was established before typically using isolated speech from single speakers (NIST SRE corpora, parliamentary speeches). On the contrary, this work applies lexical approaches for SID on a different type of data. It uses the REPERE corpus consisting of unsegmented multiparty conversations, mostly debates, discussions and Q&A sessions from TV shows. It is hypothesized that people give out clues to their identity when speaking in such settings which this work aims to exploit. The impact on SID performance of the diarization front-end required to pre-process the unsegmented data is also measured. Four lexical SID approaches are studied in this work, including TFIDF, BM25 and LDA-based topic modeling. Results are analysed in terms of TV shows and speaker roles. Lexical approaches achieve low error rates for certain speaker roles such as anchors and journalists, sometimes lower than a standard cepstral-based Gaussian Supervector -- Support Vector Machine (GSV-SVM) system. Also, in certain cases, the lexical system shows modest improvement over the cepstral-based system performance using score-level sum fusion. To highlight the potential of using lexical information not just to improve upon cepstral-based SID systems but as an independent approach in its own right, initial studies on crossmedia SID is briefly reported. Instead of using speech data as all cepstral systems require, this approach uses Wikipedia texts to train lexical speaker models which are then tested on speech transcripts to identify speakers.},
	Author = {Anindya Roy and Herv\'{e} Bredin and William Hartmann and Viet-Bac Le and Claude Barras and Jean-Luc Gauvain},
	Journal = {{Multimedia Tools and Applications}},
	Title = {{Lexical Speaker Identification in TV Shows}},
	Year = {2014},
}

# ======= 2012-09 --> 2013-08 =================================================

# SLAM 2013
@inproceedings{Poignant2013,
	Abstract = {Existing methods for unsupervised identification of speakers in TV broadcast usually rely on the output of a speaker diarization module and try to name each cluster using names provided by another source of information: we call it ``late naming''. Hence, written names extracted from title blocks tend to lead to high precision identification, although they cannot correct errors made during the clustering step. In this paper, we extend our previous ``late naming'' approach in two ways: ``integrated naming'' and ``early naming''. While ``late naming'' relies on a speaker diarization module optimized for speaker diarization, ``integrated naming'' jointly optimize speaker diarization and name propagation in terms of identification errors. ``Early naming'' modifies the speaker diarization module by adding constraints preventing two clusters with different written names to be merged together. While ``integrated naming'' yields similar identification performance as ``late naming'' (with better precision), ``early naming'' improves over this baseline both in terms of identification error rate and stability of the clustering stopping criterion.},
	Title = {{Towards a Better Integration of Written Names for Unsupervised Speakers Identification in Videos}},
	Author = {Johann Poignant and Herv{\'e} Bredin and Laurent Besacier and Georges Qu{\'e}not and Claude Barras},
	Pages = {84--89},
	Year = {2013},
	Month = {August},
	Booktitle = {SLAM 2013, First Workshop on Speech, Language and Audio for Multimedia},
	Address = {Marseille, France},
}

# SLAM 2013
@inproceedings{Bredin2013a,
	Abstract = {We describe QCompere consortium submissions to the REPERE 2013 evaluation campaign. The REPERE challenge aims at gathering four communities (face recognition, speaker identification, optical character recognition and named entity detection) towards the same goal: multimodal person recognition in TV broadcast. First, four mono-modal components are introduced (one for each foregoing community) constituting the elementary building blocks of our various submissions. Then, depending on the target modality (speaker or face recognition) and on the task (supervised or unsupervised recognition), four different fusion techniques are introduced: they can be summarized as propagation-, classifier-, rule- or graph-based approaches. Finally, their performance is evaluated on REPERE 2013 test set and their advantages and limitations are discussed.},
	Title = {{QCompere @ REPERE 2013}},
	Author = {Herv{\'e} Bredin and Johann Poignant and Guillaume Fortier and Makarand Tapaswi and Viet-Bac Le and Anindya Roy and Claude Barras and Sophie Rosset and Achintya Sarkar and Qian Yang and Hua Gao and Alexis Mignon and Jakob Verbeek and Laurent Besacier and Georges Qu{\'e}not and Hazim Kemal Ekenel and Rainer Stiefelhagen},
	Pages = {49--54},
	Year = {2013},
	Month = {August},
	Booktitle = {SLAM 2013, First Workshop on Speech, Language and Audio for Multimedia},
	Address = {Marseille, France},
}

# InterSpeech 2013
@inproceedings{Bredin2013,
	Abstract = {Most state-of-the-art approaches address speaker diarization as a hierarchical agglomerative clustering problem in the audio domain. In this paper, we propose to revisit one of them: speech turns clustering based on the Bayesian Information Criterion (a.k.a. BIC clustering). First, we show how to model it as an integer linear programming (ILP) problem.Its resolution leads to the same overall diarization error rate as standard BIC clustering but generates significantly purer speaker clusters. Then, we describe how this approach can easily be extended to the audiovisual domain and TV broadcast in particular. The straightforward integration of detected overlaid names (used to introduce guests or journalists, and obtained via video OCR) into a multimodal ILP problem yields significantly better speaker diarization results. Finally, we explain how this novel paradigm can incidentally be used for unsupervised speaker identification (i.e. not relying on any prior acoustic speaker models). Experiments on the REPERE TV broadcast corpus show that it achieves performance close to that of an oracle capable of identifying any speaker as long as their name appears on screen at least once in the video.},
	Title = {{Integer Linear Programming for Speaker Diarization and Cross-Modal Identification in TV Broadcast}},
	Author = {Herv\'{e} Bredin and Johann Poignant},
	Booktitle = {Interspeech 2013, 14th Annual Conference of the International Speech Communication Association},
	Year = {2013},
	Month = {August},
	Address = {Lyon, France},
	}


# ACM MM 2012
@inproceedings{Ercolessi2012c,
	Title = {{StoViz}: {S}tory {V}isualization of {TV} {S}eries},
	Author = {Philippe Ercolessi and Herv\'e Bredin and Christine S\'enac},
	Booktitle = {ACM MM 2012, 20th ACM International Conference on Multimedia},
	Abstract = {Recent TV series tend to have more and more complex plot. They follow the lives of numerous characters and are made of multiple intertwined stories. In this paper, we introduce StoViz, a web-based interface allowing a fast overview of this kind of episode structure, based on our plot de-interlacing system. StoViz has two main goals. First, it provides the user with a useful overview of the episode by displaying each story separately and a short abstract extracted from them. Then, it allows an efficient visual comparison of the output of any automatic plot de-interlacing algorithm with the manual annotation in terms of stories and is therefore very helpful for evaluation purposes. StoViz is available online at http://stoviz.niderb.fr.},
	Year = {2012},
	Month = {November},
	Address = {Nara, Japan},
}

# 2012-11
@inproceedings{Ercolessi2012b,
	Title = {{H}ierarchical {F}ramework for {P}lot {D}e-interlacing of {TV} {S}eries based on {S}peakers, {D}ialogues and {I}mages},
	Author = {Philippe Ercolessi and Christine S\'enac and Sandrine Mouysset and Herv\'e Bredin},
	Booktitle = {AMVA 2012, 1st ACM International Workshop on Audio and Multimedia Methods for Large-Scale Video Analysis at ACM Multimedia 2012},
	Abstract = {Since the 90s, TV series tend to introduce more and more main characters and they are often composed of multiple intertwined stories. In this paper, we propose a hierarchical framework of plot de-interlacing  which permits to cluster semantic scenes into stories: a story is a group of scenes not necessarily contiguous but showing a strong semantic relation. Each scene is described using three different modalities (based on color histograms, speaker diarization or automatic speech recognition outputs) as well as their multimodal combination. We introduce the notion of character-driven episodes as episodes where stories are emphasized by the presence or absence of characters, and we propose an automatic method, based on a social graph, to detect these episodes. Depending on whether an episode is character-driven or not, the plot-de-interlacing -which is a scene clustering- is made either through a  traditional average-link agglomerative clustering with speaker modality only, either through a  spectral clustering with the fusion of all modalities. Experiments, conducted on twenty three episodes from three quite different TV series (different lengths and formats), show that the hierarchical framework brings an improvement for all the series.},
	Year = {2012},
	Month = {November},
	Address = {Nara, Japan},
}

# 2012-10
@inproceedings{Strat2012,
	Title = {{H}ierarchical {L}ate {F}usion for {C}oncept {D}etection in {V}ideos},
	Author = {Tiberius Strat and Alexandre Benoit and Herv\'e Bredin and Georges  Qu\'enot and Patrick  Lambert},
	Booktitle = {ECCV 2012, Workshop on Information Fusion in Computer Vision for Concept Recognition},
	Abstract = {We deal with the issue of combining dozens of classifiers into a better one, for concept detection in videos. We compare three fusion approaches that share a common structure: they all start with a classifier clustering stage, continue with an intra-cluster fusion and end with an inter-cluster fusion. The main difference between them comes from the first stage. The first approach relies on a priori knowledge about the internals of each classifier (low-level descriptors and classification algorithm) to group the set of available classifiers by similarity. The second and third approaches obtain classifier similarity measures directly from their output and group them using agglomerative clustering for the second approach and community detection for the third one.},
	Year = {2012},
	Month = {October},
	Address = {Firenze, Italy},
}

# 2012-10
@inproceedings{Bredin2012b,
	Title = {{F}usion of {S}peech, {F}aces and {T}ext for {P}erson {I}dentification in {TV} {B}roadcast},
	Author = {Herv\'e Bredin and Johann Poignant and Makarand Tapaswi and Guillaume Fortier and Viet Bac Le and Thibault Napoleon and Hua Gao and Claude Barras and Sophie Rosset and Laurent Besacier and Jakob Verbeek and Georges Qu\'enot and Fr\'ed\'eric Jurie and Hazim Kemal Ekenel},
	Booktitle = {ECCV 2012, Workshop on Information Fusion in Computer Vision for Concept Recognition},
	Abstract = {The REPERE challenge is a project aiming at the evaluation of systems for supervised and unsupervised multimodal recognition of people in TV broadcast. In this paper, we describe, evaluate and discuss QCompere consortium submissions to the 2012 \repere evaluation campaign dry-run. Speaker identification (and face recognition) can be greatly improved when combined with name detection through video optical character recognition. Moreover, we show that unsupervised multimodal person recognition systems can achieve performance nearly as good as supervised monomodal ones (with several hundreds of identity models).},
	Year = {2012},
	Month = {October},
	Address = {Firenze, Italy},
}

# 2012-09
@article{Ercolessi2012a,
	Abstract = {Modern TV series have complex plots made of several intertwined stories following numerous characters. In this paper, we propose an approach for automatically detecting these stories in order to generate video summaries and we propose a visualization tool to have a quick and easy look at TV series. Based on automatic scene segmentation of each TV series episode (a scene is defined as temporally and spatially continuous and semantically coherent), scenes are clustered into stories, made of (non necessarily adjacent) semantically similar scenes. Visual, audio and text modalities are combined to achieve better scene segmentation and story detection performance. An extraction of salient scenes from stories is performed to create the summary. Experimentations are conducted on two TV series with different formats.},
	Author = {Philippe Ercolessi and Christine S\'{e}nac and Herv\'{e} Bredin and Sandrine Mouysset},
	Journal = {Document Num\'{e}rique -- Num\'{e}ro Sp\'{e}cial ``R\'{e}sum\'{e} Automatique des Documents''},
	Title = {{V}ers un {R}\'{e}sum\'{e} {A}utomatique de {S}\'{e}ries {T}\'{e}l\'{e}vis\'{e}es bas\'{e} sur une {R}echerche {M}ultimodale d'{H}istoires},
	Year = {2012},
}

# 2012-09
@inproceedings{Poignant2012,
	Abstract = {We propose an approach for unsupervised speaker identification in TV broadcast videos, by combining acoustic speaker diarization with person names obtained via video OCR from overlaid texts. Three methods for the propagation of the overlaid names to the speech turns are compared, taking into account the co-occurence duration between the speaker clusters and the names provided by the video OCR and using a task-adapted variant of the TF-IDF information retrieval coefficient. These methods were tested on the REPERE dry-run evaluation corpus, containing 3 hours of annotated videos. Our best unsupervised system reaches a F-measure of 70.2\% when considering all the speakers, and 81.7\% if anchor speakers are left out. By comparison, a mono-modal, supervised speaker identification system with 535 speaker models trained on matching development data and additional TV and radio data only provided a 57.5\% F-measure when considering all the speakers and 45.7\% without anchor.},
	Address = {Portland, Oregon},
	Author = {Johann Poignant and Herv\'{e} Bredin and Viet-Bac Le and Laurent Besacier and Claude Barras and Georges Qu\'{e}not},
	Booktitle = {Interspeech 2012, 13th Annual Conference of the International Speech Communication Association},
	Month = {September},
	Title = {{U}nsupervised {S}peaker {I}dentification using {O}verlaid {T}exts in {TV} {B}roadcast},
	Year = {2012},
}

# ======= 2011-09 --> 2012-08 =================================================

# 2012-06
@inproceedings{Ercolessi2012,
	Abstract = {Multiple sub-stories usually coexist in every episode of a TV series. We propose several variants of an approach for plot de-interlacing based on scenes clustering -- with the ultimate goal of providing the end-user with tools for fast and easy overview of one episode, one season or the whole TV series. Each scene can be described in three different ways (based on color histograms, speaker diarization or automatic speech recognition outputs) and four clustering approaches are investigated, one of them based on a graphical representation of the video. Experiments are performed on two TV series of different lengths and formats. We show that semantic descriptors (such as speaker diarization) give the best results and underline that our approach provides useful information for plot de-interlacing.},
	Address = {Annecy, France},
	Author = {Philippe Ercolessi and Christine S\'{e}nac and Herv\'{e} Bredin},
	Booktitle = {CBMI 2012, 10th Workshop on Content-Based Multimedia Indexing},
	Month = {June},
	Title = {{T}oward {P}lot {D}e-{I}nterlacing in {TV} {S}eries using {S}cenes {C}lustering},
	Year = {2012},
}

# 2012-03
@inproceedings{Bredin2012a,
	Abstract = {We deal with the issue of combining dozens of classifiers into a better one. Our first contribution is the introduction of the notion of communities of classifiers. We build a complete graph with one node per classifier and edges weighted by a measure of similarity between connected classifiers. The resulting community structure is uncovered from this graph using the state-of-the-art Louvain algorithm. Our second contribution is a hierarchical fusion approach driven by these communities. First, intra-community fusion results in one classifier per community. Then, inter-community fusion takes advantage of their complementarity to achieve much better classification performance. Application to the combination of 90 classifiers in the framework of TRECVid 2010 Semantic Indexing task shows a 30% increase in performance relative to a baseline flat fusion.},
	Address = {Kyoto, Japan},
	Author = {Herv{\'e} Bredin},
	Booktitle = {ICASSP 2012, IEEE International Conference on Acoustics, Speech, and Signal Processing},
	Month = {March},
	Title = {{C}ommunity-driven {H}ierarchical {F}usion of {N}umerous {C}lassifiers: {A}pplication to {V}ideo {S}emantic {I}ndexing},
	Year = {2012},
}

# 2012-03
@inproceedings{Bredin2012,
	Abstract = {We investigate the use of speaker diarization (SD) and automatic speech recognition (ASR) for the segmentation of audiovisual documents into scenes. We introduce multiple monomodal and multimodal approaches based on a state-of-the-art algorithm called generalized scene transition graph (GSTG). First, we extend the latter with the use of semantic information derived from both SD and ASR. Then, multimodal fusion of color histograms, SD and ASR is investigated at various point of the GSTG pipeline (early, late or intermediate fusion). Experiments driven on a few episodes of a popular TV show indicate that SD and ASR can be successfully combined with visual information and bring an additional +11% relative increase in terms of F-Measure for scene boundary detection over the state-of-the-art baseline.},
	Address = {Kyoto, Japan},
	Author = {Herv{\'e} Bredin},
	Booktitle = {ICASSP 2012, IEEE International Conference on Acoustics, Speech, and Signal Processing},
	Month = {March},
	Title = {{S}egmentation of {TV} {S}hows into {S}cenes using {S}peaker {D}iarization and {S}peech {R}ecognition},
	Year = {2012},
}

# 2011-11
@inproceedings{Delezoide2012,
  Title = {{IRIM} at {TRECVID} 2011: {S}emantic {I}ndexing and {I}nstance {S}earch},
  Author = {Bertrand Delezoide and Fr\'ed\'eric Precioso and Philippe Gosselin and Miriam Redi and Bernard M\'erialdo and Lionel Granjon and Denis Pellerin and Mich\`ele Rombaut and Herv\'e J\'egou and R\'emi Vieux and Boris Mansencal and Jenny Benois-Pineau and St\'ephane Ayache and Bahjat Safadi and Franck Thollard and Georges Qu\'enot and Herv\'e Bredin and Matthieu Cord and Alexandre Benoit and Patrick Lambert and Tiberius Strat and Joseph Razik and S\'ebastion Paris and Herv\'e Glotin},
  Abstract = {The IRIM group is a consortium of French teams working on Multimedia Indexing and Retrieval. This paper describes its participation to the TRECVID 2011 semantic indexing and instance search tasks. For the semantic indexing task, our approach uses a six-stages processing pipelines for computing scores for the likelihood of a video shot to contain a target concept. These scores are then used for producing a ranked list of images or shots that are the most likely to contain the target concept. The pipeline is composed of the following steps: descriptor extraction, descriptor optimization, classification, fusion of descriptor variants, higher-level fusion, and re-ranking. We evaluated a number of different descriptors and tried different fusion strategies. The best IRIM run has a Mean Inferred Average Precision of 0.1387, which ranked us 5th out of 19 participants. For the instance search task, we we used both object based query and frame based query. We formulated the query in standard way as comparison of visual signatures either of object with parts of DB frames or as a comparison of visual signatures of query and DB frames. To produce visual signatures we also used two apporaches: the first one is the baseline Bag-Of-Visual-Words (BOVW) model based on SURF interest point descriptor; the second approach is a Bag-Of-Regions (BOR) model that extends the traditional notion of BOVW vocabulary not only to keypoint-based descriptors but to region based descriptors.},
  Year = {2012},
  Booktitle = {TRECVid 2011, TREC Video Retrieval Evaluation Online Proceedings},
}

# 2011-10
@article{Ramona2011,
	Abstract = {This paper presents the first public framework for the evaluation of audio fingerprinting techniques. Although the domain of audio identification is very active, both in the industry and the academic world, there is nowadays no common basis to compare the proposed techniques. This is because corpuses and evaluation protocols differ between the authors. The framework we present here corresponds to a use-case in which audio excerpts have to be detected in a radio broadcast stream. This scenario indeed naturally provides a large variety of audio distortions that makes this task a real challenge for fingerprinting systems. Scoring metrics are discussed, with regard to this particular scenario. We then describe a whole evaluation framework including an audio corpus, along with the related groundtruth annotation, and a toolkit for the computation of the score metrics. An example of application of this framework is finally detailed. This took place during the evaluation campaign of the Quaero project. This evaluation framework is publicly available for download and constitutes a simple, yet thorough, platform that can be used by the community in the field of audio identification, to encourage reproducible results.},
	Author = {Mathieu Ramona and S\'{e}bastien Fenet and Rapha\"{e}l Blouet and Herv\'{e} Bredin and Thomas Fillon and Geoffroy Peeters},
	Journal = {Applied Artificial Intelligence},
	Title = {{A} {P}ublic {A}udio {I}dentification {E}valuation {F}ramework for {B}roadcast {M}onitoring},
	Year = {2011},
}

# ======= before 2011-08 ======================================================


@inproceedings{Ercolessi2011,
	Abstract = {In this paper, we propose a novel approach to perform scene segmentation of TV series. Using the output of our existing speaker diarization system, any temporal segment of the video can be described as a binary feature vector. A straightforward segmentation algorithm then allows to group similar contiguous speaker segments into scenes. An additional visual-only color-based segmentation is then used to refine the first segmentation. Experiments are performed on a subset of the Ally McBeal TV series and show promising results, obtained with a rule-free and generic method. For comparison purposes, test corpus annotations and description are made available to the community.},
	Address = {Delft, The Netherlands},
	Author = {Philippe Ercolessi and Herv\'{e} Bredin and Christine S\'{e}nac and Philippe Joly},
	Booktitle = {WIAMIS 2011, 12th International Workshop on Image Analysis for Multimedia Interactive Services},
	Month = {April},
	Title = {{S}egmenting {TV} {S}eries into {S}cenes using {S}peaker {D}iarization},
	Year = {2011},
}

# 2010-11
@inproceedings{Gorisse2011,
  Title = {{IRIM} at {TRECVID} 2010: {S}emantic {I}ndexing and {I}nstance {S}earch},
  Author = {David Gorisse and Fr\'ed\'eric Precioso and Philippe Gosselin, Lionel Granjon and Denis Pellerin and Mich\`ele Rombaut and Herv\'e Bredin and Lionel Koenig and R\'emi Vieux and Boris Mansencal and Jenny Benois-Pineau and Hugo Boujut and Claire Morand and Herv\'e J\'egou and St\'ephane Ayache and Bahjat Safadi and Yubing Tong and Franck Thollard and Georges Qu\'enot and Matthieu Cord and Alexandre Beno\^it and Patrick Lambert},
  Year = {2011},
  Abstract = {The IRIM group is a consortium of French teams working on Multimedia Indexing and Retrieval. This paper describes our participation to the TRECVID 2010 semantic indexing and instance search tasks. For the semantic indexing task, we evaluated a number of different descriptors and tried different fusion strategies, in particular hierarchical fusion. The best IRIM run has a Mean Inferred Average Precision of 0.0442, which is above the task median performance. We found that fusion of the classification scores from different classifier types improves the performance and that even with a quite low individual performance, audio descriptors can help. For the instance search task, we used only one of the example images in our queries. The rank is nearly in the middle of the list of participants. The experiment showed that HSV features outperform the concatenation of HSV and Edge histograms or the Wavelet features.},
  Booktitle = {TRECVid 2010, TREC Video Retrieval Evaluation Online Proceedings},
}

# 2009-11
@inproceedings{Bredin2010,
  Title = {{IRIT} at {TRECVID} {HLF} 2009: {A}udio to the {R}escue},
  Author = {Herv\'e Bredin and Lionel Koenig and H\'el\`ene Lachambre and Elie El Khoury},
  Booktitle = {TRECVid 2009, TREC Video Retrieval Evaluation Online Proceedings},
  Year = {2010},
}

# 2009-11
@inproceedings{Delezoide2010,
    Title = {{IRIM} at {TRECVID} 2009: {H}igh {L}evel {F}eature {E}xtraction},
    Author = {Delezoide, Bertrand and Le Borgne, Herv{\'e} and Mo{\"e}llic, Pierre-Alain and Gorisse, David and Precioso, Fr{\'e}d{\'e}ric and Wang, Feng and Merialdo, Bernard and Gosselin, Philippe and Granjon, Lionel and Pellerin, Denis and Rombaut, Mich{\`e}le and Bredin, Herv{\'e} and Koenig, Lionel and Lachambre, H{\'e}l{\`e}ne and El Khoury, Elie and Mansencal, Boris and Zhou, Yifan and Benois-Pineau, Jenny and J{\'e}gou, Herv{\'e} and Ayache, St{\'e}phane and Safadi, Bahjat and Quenot, Georges and Fabrizio, Jonathan and Cord, Matthieu and Glotin, Herv{\'e} and Zhao, Zhongqiu and Dumont, Emilie and Augereau, Bertrand},
    Abstract = {The IRIM group is a consortium of French teams working on Multimedia Indexing and Retrieval. This paper describes our participation to the TRECVID 2009 High Level Features detection task. We evaluated a large number of different descriptors (on TRECVID 2008 data) and tried different fusion strategies, in particular hierarchical fusion and genetic fusion. The best IRIM run has a Mean Inferred Average Precision of 0.1220, which is significantly above TRECVID 2009 HLF detection task median performance. We found that fusion of the classification scores from different classifier types improves the performance and that even with a quite low individual performance, audio descriptors can help.},
  Booktitle = {TRECVid 2009, TREC Video Retrieval Evaluation Online Proceedings},
    Year = {2010},
}

@inproceedings{Cooray2009,
	Address = {New York, NY, USA},
	Author = {Saman H. Cooray and Herv\'{e} Bredin and Li-Qun Xu and Noel E. O'Connor},
	Booktitle = {ACM MM 2009, 17th ACM International Conference on Multimedia},
	Doi = {http://doi.acm.org/10.1145/1631272.1631388},
	Isbn = {978-1-60558-608-3},
	Location = {Beijing, China},
	Numpages = {4},
	Pages = {685--688},
	Publisher = {ACM},
	Series = {MM '09},
	Title = {An {I}nteractive and {M}ulti-{L}evel {F}ramework for {S}ummarising {U}ser-{G}enerated {V}ideos},
	Url = {http://doi.acm.org/10.1145/1631272.1631388},
	Year = {2009},
}

@article{Karam2009,
	Author = {Walid Karam and Herv{\'e} Bredin and Hanna Greige and G{\'e}rard Chollet and Chafic Mokbel},
	Journal = {EURASIP Journal on Advances in Signal Processing, Special Issue on Recent Advances in Biometric Systems: A Signal Processing Perspective},
	Month = {April},
	Title = {{T}alking-{F}ace {I}dentity {V}erification, {A}udiovisual {F}orgery and {R}obustness {I}ssues},
	Url = {http://dx.doi.org/10.1155/2009/746481},
	Year = {2009},
}

@inbook{Bredin2009,
	Author = {Herv{\'e} Bredin and Aur{\'e}lien Mayoue and G{\'e}rard Chollet and Bernadette Dorizzi},
	Booktitle = {Guide to Biometric Reference Systems and Performance Evaluation},
	Editor = {Dijana Petrovska and G{\'e}rard Chollet},
	Month = {February},
	Publisher = {Springer Verlag},
	Title = {{T}alking-{F}ace {A}uthentication},
	Url = {http://springer.com/978-1-84800-291-3},
	Year = {2009},
}

@inproceedings{Dumont2008a,
	Address = {Koblenz, Germany},
	Author = {Emilie Dumont and Bernard Merialdo and Slim Essid and Werner Bailer and Daragh Byrne and Herv{\'e} Bredin and Noel O'Connor and Gareth JF Jones and Martin Haller and Andreas Krutz and Thomas Sikora and Tomas Piatrik},
	Booktitle = {SAMT 2008, 3rd International Conference on Semantic and Digital Media Technologies},
	Month = {December},
	Title = {A {C}ollaborative {A}pproach to {V}ideo {S}ummarization},
	Year = {2008},
}

@inproceedings{Bredin2008a,
	Address = {Vancouver, BC, Canada},
	Author = {Herv{\'e} Bredin and Daragh Byrne and Hyowon Lee and Noel O'Connor and Gareth JF Jones},
	Booktitle = {TRECVID 2008, ACM International Conference on Multimedia Information Retrieval 2008},
	Month = {November},
	Title = {{D}ublin {C}ity {U}niversity at {TRECVid} 2008 {BBC} {R}ushes {S}ummarisation {T}ask},
	Year = {2008},
}

@inproceedings{Dumont2008,
	Address = {Vancouver, BC, Canada},
	Author = {Emilie Dumont and Bernard Merialdo and Slim Essid and Werner Bailer and Herwig Rehatschek and Daragh Byrne and Herv{\'e} Bredin and Noel O'Connor and Gareth JF Jones and Alan F Smeaton and Martin Haller and Andreas Krutz and Thomas Sikora and Tomas Piatrik},
	Booktitle = {TRECVID 2008, ACM International Conference on Multimedia Information Retrieval},
	Month = {November},
	Title = {{R}ushes {V}ideo {S}ummarization using a {C}ollaborative {A}pproach},
	Year = {2008},
}

@inproceedings{Fauve2008,
	Address = {Las Vegas, USA},
	Author = {Beno\"{i}t Fauve and Herv{\'e} Bredin and Walid Karam and Florian Verdet and Aur{\'e}lien Mayoue and G{\'e}rard Chollet and Jean Hennebert and R. Lewis and John Mason and Chafic Mokbel and Dijana Petrovska},
	Booktitle = {ICASSP 2008, IEEE International Conference on Acoustics, Speech, and Signal Processing},
	Month = {April},
	Title = {{S}ome {R}esults from the {BioSecure} {T}alking-{F}ace {E}valuation {C}ampaign},
	Year = {2008},
}

@inproceedings{Bredin2008,
	Address = {Las Vegas, USA},
	Author = {Herv{\'e} Bredin and G{\'e}rard Chollet},
	Booktitle = {ICASSP 2008, IEEE International Conference on Acoustics, Speech, and Signal Processing},
	Month = {April},
	Title = {{M}aking {T}alking-{F}ace {A}uthentication {R}obust to {D}eliberate {I}mposture},
	Year = {2008},
}

@inbook{Abboud2007,
	Author = {Bouchra Abboud and Herv{\'e} Bredin and Guido Aversano and G{\'e}rard Chollet},
	Booktitle = {Progress in Nonlinear Speech Processing},
	Editor = {Yannis Stylianou},
	Pages = {118-134},
	Publisher = {Springer Verlag},
	Title = {{A}udio-{V}isual {I}dentity {V}erification: an {I}ntroductory {O}verview},
	Volume = {4391},
	Year = {2007},
}

@inbook{Chollet2007,
	Author = {G{\'e}rard Chollet and R{\'e}mi Landais and Herv{\'e} Bredin and Thomas Hueber and Chafic Mokbel and Patrick Perrot and Leila Zouari},
	Booktitle = {Non-Linear Speech Processing},
	Editor = {Mohamed Chetouani},
	Publisher = {Springer Verlag},
	Title = {{S}ome {E}xperiments in {A}udio-{V}isual {S}peech {P}rocessing},
	Year = {2007},
}

@article{Argones-Rua2007a,
	Author = {Enrique Argones-R{\'u}a and Herv{\'e} Bredin and Carmen Garc{\'i}a-Mateo and G{\'e}rard Chollet and Daniel Gonz{\'a}lez-Jim{\'e}nez},
	Journal = {Pattern Analysis and Applications Journal},
	Month = {December},
	Title = {{A}udio-{V}isual {S}peech {A}synchrony {D}etection using {C}o-{I}nertia {A}nalysis and {C}oupled {H}idden {M}arkov {M}odels},
	Year = {2007},
}

@phdthesis{Bredin2007b,
	Address = {Paris, France},
	Author = {Herv{\'e} Bredin},
	Month = {November},
	School = {T{\'e}l{\'e}com ParisTech},
	Title = {{V}{\'e}rification de l'{I}dentit{\'e} d'un {V}isage {P}arlant. {A}pport de la {M}esure de {S}ynchronie {A}udiovisuelle face aux {T}entatives {D}{\'e}lib{\'e}r{\'e}es d'{I}mposture.},
	Type = {{PhD}},
	Year = {2007},
}

@inproceedings{Perrot2007,
	Address = {London, UK},
	Author = {Patrick Perrot and Herv{\'e} Bredin and G{\'e}rard Chollet},
	Booktitle = {2007 International Crime Science Conference},
	Month = {July},
	Title = {{B}iometrics and {F}orensic {S}ciences: the {S}ame {Q}uest for {I}dentification?},
	Year = {2007},
}

@inproceedings{Argones-Rua2007,
	Address = {Girona, Spain},
	Author = {Enrique Argones-R{\'u}a and Carmen Garc{\'i}a-Mateo and Herv{\'e} Bredin and G{\'e}rard Chollet},
	Booktitle = {1st Spanish Workshop on Biometrics},
	Month = {June},
	Title = {{A}liveness {D}etection using {C}oupled {H}idden {M}arkov {M}odels},
	Year = {2007},
}

@inproceedings{Landais2007,
	Address = {Hammamet, Tunisia},
	Author = {R{\'e}mi Landais and Herv{\'e} Bredin and Leila Zouari and G{\'e}rard Chollet},
	Booktitle = {Traitement et Analyse de l'Information : M{\'e}thodes et Applications},
	Month = {May},
	Title = {{V}{\'e}rification {A}udiovisuelle de l'{I}dentit{\'e}},
	Year = {2007},
}

@inproceedings{Bredin2007a,
	Address = {Honolulu, Hawaii, USA},
	Author = {Herv{\'e} Bredin and G{\'e}rard Chollet},
	Booktitle = {ICASSP 2007, IEEE International Conference on Acoustics, Speech, and Signal Processing},
	Month = {April},
	Title = {{A}udio-{V}isual {S}peech {S}ynchrony {M}easure for {T}alking-{F}ace {I}dentity {V}erification},
	Year = {2007},
}

@article{Bredin2007,
	Author = {Herv{\'e} Bredin and G{\'e}rard Chollet},
	Chapter = {179},
	Journal = {EURASIP Journal on Advances in Signal Processing, Special Issue on Knowledge-Assisted Media Analysis for Interactive Multimedia Applications},
	Month = {January},
	Title = {{A}udio-{V}isual {S}peech {S}ynchrony {M}easure: {A}pplication to {B}iometrics},
	Volume = {1},
	Year = {2007},
}

@inproceedings{Bredin2006c,
	Address = {Bangalore, India},
	Author = {Herv{\'e} Bredin and G{\'e}rard Chollet},
	Booktitle = {VIE 2006, IEE International Conference on Visual Information Engineering},
	Month = {September},
	Title = {{M}easuring {A}udio and {V}isual {S}peech {S}ynchrony: {M}ethods and {A}pplications},
	Year = {2006},
}

@inproceedings{Bredin2006b,
	Address = {Hong-Kong, China},
	Author = {Herv{\'e} Bredin and Najim Dehak and G{\'e}rard Chollet},
	Booktitle = {ICPR 2006, IAPR International Conference on Pattern Recognition},
	Month = {August},
	Title = {{GMM}-based {SVM} for {F}ace {R}ecognition},
	Year = {2006},
}

@inproceedings{Brugger2006,
	Address = {Dinard, France},
	Author = {Fabian Brugger and Leila Zouari and Herv{\'e} Bredin and Asma Amehraye and G{\'e}rard Chollet and Dominique Pastor and Yang Ni},
	Booktitle = {JEP 2006, Journ{\'e}es d'Etudes sur la Parole},
	Month = {June},
	Title = {{R}econnaissance {A}udiovisuelle de la {P}arole par {VMike}},
	Year = {2006},
}

@inproceedings{Bredin2006a,
	Address = {Toulouse, France},
	Author = {Herv{\'e} Bredin and Guido Aversano and Chafic Mokbel and G{\'e}rard Chollet},
	Booktitle = {MMUA 2006, Workshop on Multimodal User Authentication},
	Month = {May},
	Title = {{T}he {BioSecure} {T}alking-{F}ace {R}eference {S}ystem},
	Year = {2006},
}

@inproceedings{Koreman2006,
	Address = {Toulouse, France},
	Author = {Jacques Koreman and Andrew C Morris and D. Wu and Sabah Jassim and Harin Sellahewa and J. Ehlers and G{\'e}rard Chollet and Guido Aversano and Herv{\'e} Bredin and Sonia Garcia-Salicetti and Lor{\`e}ne Allano and Bao Ly Van and Bernadette Dorizzi},
	Booktitle = {MMUA 2006, Workshop on Multimodal User Authentication},
	Month = {May},
	Title = {{M}ulti-{M}odal {B}iometric {A}uthentication on the {SecurePhone} {PDA}},
	Year = {2006},
}

@inproceedings{Bredin2006,
	Address = {Toulouse, France},
	Author = {Herv{\'e} Bredin and Antonio Miguel and Ian Witten and G{\'e}rard Chollet},
	Booktitle = {ICASSP 2006, IEEE International Conference on Acoustics, Speech, and Signal Processing},
	Month = {May},
	Title = {{D}etecting {R}eplay {A}ttacks in {A}udiovisual {I}dentity {V}erification},
	Year = {2006},
}

@inproceedings{McTait2005,
	Address = {Zaghreb, Croatia},
	Author = {Kevin McTait and Herv{\'e} Bredin and Silvia Col{\'o}n and Thomas Fillon and G{\'e}rard Chollet},
	Booktitle = {ISISPA 2005, International Symposium on Image and Signal Processing and Analysis},
	Month = {September},
	Title = {{A}dapting a {H}igh {Q}uality {A}udiovisual {D}atabase to {PDA} {Q}uality},
	Year = {2005},
}
